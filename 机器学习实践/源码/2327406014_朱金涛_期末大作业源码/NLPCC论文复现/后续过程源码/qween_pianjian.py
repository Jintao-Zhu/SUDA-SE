import os
import json
import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoConfig,
    AutoTokenizer,
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType

# ================= é…ç½®åŒºåŸŸ =================
# âœ… æ¨¡å‹æœ¬åœ°ç»å¯¹è·¯å¾„ (è¿™æ˜¯ä½ åˆšåˆšä¸‹è½½å¥½çš„ä½ç½®)
MODEL_PATH = "/public/home/lilingzhi/Qwen2.5-7B-Instruct"

# âœ… æ•°æ®æ–‡ä»¶ (å°±åœ¨å½“å‰ç›®å½•ä¸‹)
DATA_FILE = "pianjian_cot_backup.json" 

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "qwen_lora_outputs_ddp"

# ================= 1. ç¯å¢ƒä¸æ•°æ®æ£€æŸ¥ =================
# åªåœ¨ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—ï¼Œé˜²æ­¢åŒå¡åˆ·å±
local_rank = int(os.environ.get("LOCAL_RANK", 0))

if local_rank == 0:
    print("=== ç¯å¢ƒæ£€æŸ¥ ===")
    if torch.cuda.is_available():
        print(f"âœ… å‘ç° GPU æ•°é‡: {torch.cuda.device_count()}")
        print(f"âœ… å½“å‰æ˜¾å¡: {torch.cuda.get_device_name(0)}")
    else:
        raise RuntimeError("âŒ æœªæ£€æµ‹åˆ° GPUï¼")
    
    print(f"\n=== æ­£åœ¨åŠ è½½æ•°æ®: {DATA_FILE} ===")

if not os.path.exists(DATA_FILE):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {DATA_FILE}")

try:
    # å°è¯•æŒ‰ JSON Lines æ ¼å¼åŠ è½½
    df = pd.read_json(DATA_FILE, lines=True)
except ValueError:
    # å¤±è´¥åˆ™å°è¯•æ ‡å‡† JSON
    df = pd.read_json(DATA_FILE)

dataset = Dataset.from_pandas(df)

if local_rank == 0:
    print(f"âœ… æˆåŠŸè¯»å– {len(dataset)} æ¡æ•°æ®")

# ================= 2. åŠ è½½æ¨¡å‹ (åŸç”Ÿ DDP æ¨¡å¼) =================
if local_rank == 0:
    print(f"\n=== æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹: {MODEL_PATH} ===")

try:
    # ğŸ› ï¸ å…³é”®ä¿®å¤ï¼šæ‰‹åŠ¨åŠ è½½ Configï¼Œé˜²æ­¢è‡ªåŠ¨è¯†åˆ«å‡ºé”™
    config = AutoConfig.from_pretrained(MODEL_PATH, local_files_only=True)
    
    # åŠ è½½ Tokenizer
    # æ³¨æ„ï¼šä¸å†ä½¿ç”¨ trust_remote_code=Trueï¼Œé¿å…åŠ è½½é”™è¯¯ä»£ç 
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH, 
        config=config,
        local_files_only=True
    )
    tokenizer.pad_token = tokenizer.eos_token 

    # åŠ è½½æ¨¡å‹
    # âš ï¸ å…³é”®ï¼šDDP æ¨¡å¼ä¸‹ç»å¯¹ä¸èƒ½å†™ device_map="auto"
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        config=config,
        torch_dtype=torch.float16, # V100 å®Œç¾æ”¯æŒ FP16
        local_files_only=True
    )
    
    if local_rank == 0:
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

except Exception as e:
    raise RuntimeError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\né”™è¯¯ä¿¡æ¯: {e}")

# ================= 3. é…ç½® LoRA =================
if local_rank == 0:
    print("\n=== æ­£åœ¨é…ç½® LoRA ===")

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    inference_mode=False, 
    r=16, 
    lora_alpha=32, 
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)
model = get_peft_model(model, peft_config)

# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ (å¤§å¹…èŠ‚çœæ˜¾å­˜ï¼Œé˜²æ­¢ OOM)
model.gradient_checkpointing_enable() 
model.enable_input_require_grads()

if local_rank == 0:
    model.print_trainable_parameters()

# ================= 4. æ•°æ®æ ¼å¼åŒ– =================
if local_rank == 0:
    print("\n=== æ­£åœ¨æ ¼å¼åŒ–æ•°æ® ===")

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are an expert in gender bias mitigation. Please analyze the following text for gender bias, provide a step-by-step chain-of-thought analysis, classify the bias type, and provide a rewritten version if bias exists.

### Input:
{}

### Response:
{}"""

def preprocess_function(examples):
    inputs = examples["original_text"]
    targets = examples["Bias_Analysis_CoT"]
    
    model_inputs = []
    for i in range(len(inputs)):
        prompt = alpaca_prompt.format(inputs[i], "")
        full_text = prompt + str(targets[i]) + tokenizer.eos_token
        
        tokenized = tokenizer(
            full_text,
            truncation=True,
            max_length=2048,
            padding="max_length",
        )
        tokenized["labels"] = tokenized["input_ids"].copy()
        model_inputs.append(tokenized)
        
    return {k: [d[k] for d in model_inputs] for k in model_inputs[0].keys()}

tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)

# ================= 5. å¼€å§‹è®­ç»ƒ (åŒå¡å‚æ•°ä¼˜åŒ–) =================
if local_rank == 0:
    print("\n=== å¼€å§‹åŒå¡è®­ç»ƒ ===")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    # æ˜¾å­˜ç­–ç•¥ï¼šV100 32G å¾ˆå¤§ï¼Œä½†ä¹Ÿç»ä¸ä½ Qwen-7B éšä¾¿é€ 
    # å•å¡ batch size è®¾ä¸º 2ï¼Œé…åˆæ¢¯åº¦ç´¯ç§¯ 8
    # æ€» batch size = 2 (å•å¡) * 2 (å¡æ•°) * 8 (ç´¯ç§¯) = 32
    per_device_train_batch_size=2, 
    gradient_accumulation_steps=8, 
    learning_rate=2e-4,
    num_train_epochs=3,
    logging_steps=5,
    fp16=True, # å¼€å¯åŠç²¾åº¦åŠ é€Ÿ
    save_strategy="epoch",
    optim="adamw_torch", # ä½¿ç”¨åŸç”Ÿä¼˜åŒ–å™¨ï¼Œä¸ä¾èµ– bitsandbytes
    report_to="none",
    ddp_find_unused_parameters=False, # DDP å¿…é¡»å‚æ•°
    gradient_checkpointing=True, # å¿…é¡»å¼€å¯ï¼Œå¦åˆ™å®¹æ˜“ OOM
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True),
)

trainer.train()

# ================= 6. ä¿å­˜æ¨¡å‹ =================
# åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ï¼Œé˜²æ­¢å†™å†²çª
if local_rank == 0:
    print(f"\n=== è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜åˆ° {OUTPUT_DIR} ===")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")