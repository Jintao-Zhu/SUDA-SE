import json
import re
import ast
import numpy as np
import jieba 
from sklearn.metrics import f1_score
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.meteor_score import meteor_score
import nltk
from tqdm import tqdm

# ==================== åˆå§‹åŒ– ====================
try:
    # å°è¯•åŠ è½½ wordnet (å¦‚æœæ²¡ç½‘ä¼šè·³è¿‡ï¼Œä¸å½±å“å…¶ä»–æŒ‡æ ‡)
    nltk.data.find('corpora/wordnet')
except LookupError:
    pass

# ==================== æ ¸å¿ƒç®—æ³•å‡½æ•° ====================
def my_rouge_l(ref_tokens, cand_tokens):
    """æ‰‹å†™ ROUGE-L (LCSç®—æ³•)ï¼Œè§£å†³ç¬¬ä¸‰æ–¹åº“ä¸­æ–‡å…¼å®¹æ€§é—®é¢˜"""
    if not ref_tokens or not cand_tokens: return 0.0
    m, n = len(ref_tokens), len(cand_tokens)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref_tokens[i - 1] == cand_tokens[j - 1]:
                dp[i][j] = dp[i - 1][j - 1] + 1
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
    lcs_len = dp[m][n]
    prec = lcs_len / n if n > 0 else 0
    rec = lcs_len / m if m > 0 else 0
    if prec + rec == 0: return 0.0
    return 2 * prec * rec / (prec + rec)

def clean_and_parse(text_str):
    """å¼ºåŠ›æ¸…æ´—å‡½æ•° (ç”¨äºå¤„ç†æ¨¡å‹ç”Ÿæˆçš„ Raw String)"""
    if not isinstance(text_str, str): return {}
    start = text_str.find("{")
    end = text_str.rfind("}") + 1
    if start == -1 or end == -1 or start >= end: return None
    clean_str = text_str[start:end]
    pattern = r'("[^"]*")|#.*'
    clean_str = re.sub(pattern, lambda m: m.group(1) if m.group(1) else "", clean_str)
    python_style_str = (clean_str.replace("true", "True").replace("false", "False").replace("null", "None"))
    try: return json.loads(clean_str)
    except: pass
    try: return ast.literal_eval(python_style_str)
    except: pass
    return None

def tokenize_zh(text):
    return list(jieba.cut(text))

# ==================== ä¸»é€»è¾‘ ====================
RESULTS_FILE = "benchmark_results_test.json"

print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {RESULTS_FILE}")
with open(RESULTS_FILE, "r", encoding="utf-8") as f:
    data = json.load(f)

y_true_cls, y_pred_cls = [], []
scores_bleu, scores_meteor, scores_rouge = [], [], []
valid_count = 0

print(f"æ­£åœ¨è¯„ä¼° {len(data)} æ¡æµ‹è¯•é›†æ ·æœ¬...")

for idx, item in enumerate(tqdm(data)):
    # =======================================================
    # ğŸ› ï¸ å…³é”®ä¿®å¤ï¼šä»åˆ†æ•£çš„å­—æ®µä¸­é‡å»º Ground Truth
    # =======================================================
    gt_labels = item.get('ground_truth_label')
    gt_edit = item.get('ground_truth_edit')
    
    # è·å–æ¨¡å‹é¢„æµ‹ (ä¼˜å…ˆä½¿ç”¨å·²è§£æå¥½çš„ model_parsed)
    gen_json = item.get('model_parsed')
    if not gen_json:
        # å¦‚æœä¹‹å‰è§£æå¤±è´¥ï¼Œå†è¯•ä¸€æ¬¡æ¸…æ´—è§£æ
        gen_json = clean_and_parse(item.get('model_output_raw', ''))

    # åªè¦ Ground Truth æœ‰æ ‡ç­¾ï¼Œæˆ‘ä»¬å°±è¿›è¡Œè¯„ä¼°
    if gt_labels is None: 
        continue
        
    valid_count += 1

    try:
        # --- Task 2: Classification ---
        # å®¹é”™ï¼šå¦‚æœæ¨¡å‹æ²¡ç”Ÿæˆ bias_labelsï¼Œæˆ–è€…æ ¼å¼ä¸å¯¹ï¼Œé»˜è®¤ä¸º [0,0,0]
        if gen_json and isinstance(gen_json.get('bias_labels'), list) and len(gen_json['bias_labels']) >= 3:
            pred_l = gen_json['bias_labels'][:3]
        else:
            pred_l = [0, 0, 0] # æƒ©ç½šï¼šæ ¼å¼é”™è¯¯ç®—å…¨é”™
            
        y_true_cls.append(gt_labels[:3])
        y_pred_cls.append(pred_l)

        # --- Task 3: Mitigation ---
        # Ground Truth æ”¹å†™å¥
        ref_text = str(gt_edit) if gt_edit else ""
        
        # æ¨¡å‹ç”Ÿæˆæ”¹å†™å¥
        cand_text = ""
        if gen_json:
            cand_text = str(gen_json.get('edit_sentence', ''))
        
        # åªæœ‰å½“å‚è€ƒç­”æ¡ˆå­˜åœ¨æ—¶æ‰è®¡ç®—ç”ŸæˆæŒ‡æ ‡
        if ref_text:
            # 1. BLEU
            # é˜²æ­¢ç©ºå­—ç¬¦ä¸²æŠ¥é”™
            if not cand_text: cand_text = " " 
            scores_bleu.append(sentence_bleu([list(ref_text)], list(cand_text)))
            
            # åˆ†è¯ (ç”¨äº METEOR å’Œ ROUGE)
            ref_tok = tokenize_zh(ref_text)
            cand_tok = tokenize_zh(cand_text)
            
            # 2. METEOR
            try:
                scores_meteor.append(meteor_score([ref_tok], cand_tok))
            except:
                pass # NLTK æ²¡ç½‘æˆ–æŠ¥é”™å°±è·³è¿‡

            # 3. ROUGE-L
            scores_rouge.append(my_rouge_l(ref_tok, cand_tok))
            
    except Exception as e:
        # print(f"æ ·æœ¬ {idx} è®¡ç®—å‡ºé”™: {e}")
        pass

# ==================== è¾“å‡ºæœ€ç»ˆæˆç»©å• ====================
print("\n" + "="*50)
print("ğŸ“Š æµ‹è¯•é›†æœ€ç»ˆæˆç»©å• (Test Set)")
print("="*50)
print(f"æœ‰æ•ˆè¯„ä¼°æ ·æœ¬æ•°: {valid_count} / {len(data)}")

if valid_count > 0:
    # Task 2
    y_true_cls = np.array(y_true_cls)
    y_pred_cls = np.array(y_pred_cls)
    f1_list = []
    for i in range(3):
        f1 = f1_score(y_true_cls[:, i], y_pred_cls[:, i], average='macro', zero_division=0)
        f1_list.append(f1)
    print(f"ã€Task 2 - åˆ†ç±»ã€‘ Macro-F1: {np.mean(f1_list):.4f}")
    print(f"   (æ³¨: è®ºæ–‡SOTAä¸º 0.509)")

    # Task 3
    meteor_avg = np.mean(scores_meteor) if scores_meteor else 0.0
    print(f"\nã€Task 3 - ç¼“è§£ã€‘")
    print(f"  BLEU:      {np.mean(scores_bleu):.4f}")
    print(f"  METEOR:    {meteor_avg:.4f}")
    print(f"  ROUGE-L:   {np.mean(scores_rouge):.4f}")
    print(f"   (æ³¨: è®ºæ–‡BLEUä¸º0.013, ROUGEä¸º0.453)")
    print("="*50)
else:
    print("âŒ ä¾ç„¶æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ benchmark_results_test.json å†…å®¹æ˜¯å¦æ­£å¸¸ã€‚")