import os
import json
import torch
import re
import ast
import jieba
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen2Config
from peft import PeftModel
from sklearn.metrics import f1_score
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.meteor_score import meteor_score
import nltk

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹ä¸é€‚é…å™¨è·¯å¾„
BASE_MODEL_PATH = "/public/home/lilingzhi/Qwen2.5-7B-Instruct"
ADAPTER_PATH = "qwen_lora_outputs_ddp"

# 2. éªŒè¯é›†è·¯å¾„
# å¦‚æœæ‰¾ä¸åˆ°ï¼Œå¯ä»¥å°è¯•å†™ç»å¯¹è·¯å¾„
VALID_FILE = "NLPCC-2025-Shared-Task-7-main/data/test_gt/classification and mitigation/biased.json"

# 3. ç»“æœä¿å­˜è·¯å¾„
RESULT_FILE = "benchmark_results_test.json"

# ================= åˆå§‹åŒ– NLTK =================
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    pass

# ================= æ ¸å¿ƒç®—æ³•å‡½æ•° =================

def my_rouge_l(ref_tokens, cand_tokens):
    """æ‰‹å†™ ROUGE-L (LCSç®—æ³•)"""
    if not ref_tokens or not cand_tokens: return 0.0
    m, n = len(ref_tokens), len(cand_tokens)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref_tokens[i - 1] == cand_tokens[j - 1]:
                dp[i][j] = dp[i - 1][j - 1] + 1
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
    lcs_len = dp[m][n]
    prec = lcs_len / n if n > 0 else 0
    rec = lcs_len / m if m > 0 else 0
    if prec + rec == 0: return 0.0
    return 2 * prec * rec / (prec + rec)

def clean_and_parse(text_str):
    """å¼ºåŠ›æ¸…æ´—å¹¶è§£ææ¨¡å‹è¾“å‡ºçš„ JSON"""
    if not isinstance(text_str, str): return {}
    start = text_str.find("{")
    end = text_str.rfind("}") + 1
    if start == -1 or end == -1 or start >= end: return None
    clean_str = text_str[start:end]
    pattern = r'("[^"]*")|#.*'
    clean_str = re.sub(pattern, lambda m: m.group(1) if m.group(1) else "", clean_str)
    # æ›¿æ¢ Python å…³é”®å­—
    python_style_str = (clean_str
                        .replace("true", "True")
                        .replace("false", "False")
                        .replace("null", "None"))
    try: return json.loads(clean_str)
    except: pass
    try: return ast.literal_eval(python_style_str)
    except: pass
    return None

def tokenize_zh(text):
    return list(jieba.cut(text))

# ================= ä¸»æµç¨‹ =================

def main():
    # âœ… ä¿®å¤ï¼šä½¿ç”¨å±€éƒ¨å˜é‡ target_fileï¼Œé¿å… UnboundLocalError
    target_file = VALID_FILE
    
    # 1. åŠ è½½éªŒè¯é›†
    print(f"=== 1. æ­£åœ¨åŠ è½½éªŒè¯é›†: {target_file} ===")
    
    if not os.path.exists(target_file):
        # å°è¯•æ‹¼æ¥ç»å¯¹è·¯å¾„ä½œä¸ºå¤‡é€‰
        abs_path = os.path.join(os.getcwd(), target_file)
        if os.path.exists(abs_path):
            target_file = abs_path
        else:
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {target_file}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
            
    with open(target_file, 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(val_data)} æ¡éªŒè¯æ ·æœ¬")

    # 2. åŠ è½½æ¨¡å‹
    print("\n=== 2. æ­£åœ¨åŠ è½½æ¨¡å‹ä¸é€‚é…å™¨ ===")
    config = Qwen2Config.from_pretrained(BASE_MODEL_PATH, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, local_files_only=True)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        config=config,
        torch_dtype=torch.float16,
        device_map="auto",
        local_files_only=True
    )
    
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•")

    # 3. å¼€å§‹æ¨ç†ä¸å®æ—¶è¯„ä¼°
    print("\n=== 3. å¼€å§‹åŸºå‡†æµ‹è¯• (Benchmark) ===")
    
    alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are an expert in gender bias mitigation. Please analyze the following text for gender bias, provide a step-by-step chain-of-thought analysis, classify the bias type, and provide a rewritten version if bias exists.

### Input:
{}

### Response:
"""
    
    results = []
    y_true_cls, y_pred_cls = [], []
    scores_bleu, scores_rouge, scores_meteor = [], [], [] # è¡¥å…¨ meteor åˆ—è¡¨
    parse_fail = 0
    
    for item in tqdm(val_data, desc="æ¨ç†è¿›åº¦"):
        input_text = item['ori_sentence']
        
        # æ„é€  Prompt
        prompt = alpaca_prompt.format(input_text)
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=512,
                temperature=0.3, 
                top_p=0.9,
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        gen_text = response.split("### Response:")[-1].strip()
        
        # è§£æç»“æœ
        gen_json = clean_and_parse(gen_text)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results.append({
            "original_text": input_text,
            "ground_truth_label": item.get('bias_labels'),
            "ground_truth_edit": item.get('edit_sentence'),
            "model_output_raw": gen_text,
            "model_parsed": gen_json
        })
        
        if not gen_json:
            parse_fail += 1
            continue
            
        # --- å®æ—¶è®¡ç®—æŒ‡æ ‡ ---
        # Task 2: Classification
        gt_l = item.get('bias_labels', [0,0,0])
        pred_l = gen_json.get('bias_labels', [0,0,0])
        
        if not isinstance(gt_l, list) or len(gt_l) < 3: gt_l = [0,0,0]
        if not isinstance(pred_l, list) or len(pred_l) < 3: pred_l = [0,0,0]
        
        y_true_cls.append(gt_l[:3])
        y_pred_cls.append(pred_l[:3])
        
        # Task 3: Mitigation
        ref_text = str(item.get('edit_sentence', ''))
        cand_text = str(gen_json.get('edit_sentence', ''))
        
        if ref_text and cand_text:
            scores_bleu.append(sentence_bleu([list(ref_text)], list(cand_text)))
            ref_tok = tokenize_zh(ref_text)
            cand_tok = tokenize_zh(cand_text)
            
            try:
                scores_meteor.append(meteor_score([ref_tok], cand_tok))
            except:
                pass
                
            scores_rouge.append(my_rouge_l(ref_tok, cand_tok))

    # 4. ä¿å­˜è¯¦ç»†æ•°æ®
    with open(RESULT_FILE, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # 5. è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*50)
    print(f"ğŸ† éªŒè¯é›†æœ€ç»ˆæˆç»© (æ ·æœ¬æ•°: {len(val_data)})")
    print(f"è§£ææˆåŠŸç‡: {len(val_data) - parse_fail} / {len(val_data)}")
    print("="*50)
    
    if len(y_true_cls) > 0:
        y_true_cls = np.array(y_true_cls)
        y_pred_cls = np.array(y_pred_cls)
        f1_list = []
        for i in range(3):
            f1 = f1_score(y_true_cls[:, i], y_pred_cls[:, i], average='macro', zero_division=0)
            f1_list.append(f1)
        print(f"ã€Task 2 - åè§åˆ†ç±»ã€‘ Macro-F1: {np.mean(f1_list):.4f}")
    
    if scores_bleu:
        meteor_avg = np.mean(scores_meteor) if scores_meteor else 0.0
        print(f"ã€Task 3 - åè§ç¼“è§£ã€‘")
        print(f"  BLEU:    {np.mean(scores_bleu):.4f}")
        print(f"  METEOR:  {meteor_avg:.4f}")
        print(f"  ROUGE-L: {np.mean(scores_rouge):.4f}")
    
    print("="*50)
    print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {RESULT_FILE}")

if __name__ == "__main__":
    main()