import os
import json
import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoConfig,
    AutoTokenizer,
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, PeftModel # ç¡®ä¿å¯¼å…¥ PeftModel

# ================= é…ç½®åŒºåŸŸ =================
MODEL_PATH = "/public/home/lilingzhi/Qwen2.5-7B-Instruct"
DATA_FILE = "pianjian_cot_backup.json" 

# âš ï¸ å…³é”®ï¼šç»§ç»­ä½¿ç”¨æ—§çš„è¾“å‡ºç›®å½•ï¼Œå› ä¸ºæ¨¡å‹æƒé‡å°±åœ¨è¿™é‡Œ
OUTPUT_DIR = "qwen_lora_outputs_ddp" 
# æ‰¾åˆ°æœ€å¤§çš„ checkpoint ç›®å½•è·¯å¾„ (æ¨¡å‹æƒé‡å°±åœ¨è¿™é‡Œ)
LATEST_CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, "checkpoint-93") 

# ================= 1. åŠ è½½æ•°æ® =================
local_rank = int(os.environ.get("LOCAL_RANK", 0))
if local_rank == 0:
    print(f"=== æ­£åœ¨å‡†å¤‡æ–­ç‚¹ç»­è®­ (ç›®æ ‡ï¼šç»­è·‘ 7 è½®) ===")

if not os.path.exists(DATA_FILE):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {DATA_FILE}")

try:
    df = pd.read_json(DATA_FILE, lines=True)
except ValueError:
    df = pd.read_json(DATA_FILE)
dataset = Dataset.from_pandas(df)

# ================= 2. åŠ è½½æ¨¡å‹ (å…³é”®ä¿®å¤ï¼šæ‰‹åŠ¨åŠ è½½æƒé‡) =================
try:
    if local_rank == 0:
        print(f"âœ… æ‰¾åˆ° Checkpoint: {LATEST_CHECKPOINT_PATH}")
        
    config = AutoConfig.from_pretrained(MODEL_PATH, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, config=config, local_files_only=True)
    tokenizer.pad_token = tokenizer.eos_token 
    
    # 1. åŠ è½½åº•åº§æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        config=config,
        torch_dtype=torch.float16, 
        local_files_only=True
    )

    # 2. ä» Checkpoint è·¯å¾„åŠ è½½ PEFT é€‚é…å™¨æƒé‡ (æ¨¡å‹ç°åœ¨å·²ç»æ˜¯æœ€æ–°çš„äº†)
    model = PeftModel.from_pretrained(base_model, LATEST_CHECKPOINT_PATH)
    
    if local_rank == 0:
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼")

except Exception as e:
    raise RuntimeError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

# ================= 3. é…ç½® LoRA (ä¿æŒä¸å˜) =================
# PEFT é€‚é…å™¨å·²ç»åŠ è½½ï¼Œè¿™é‡Œåªéœ€è¦é…ç½®å‚æ•°ï¼Œä¸å†è°ƒç”¨ get_peft_model
model.gradient_checkpointing_enable() 
model.enable_input_require_grads()

if local_rank == 0:
    model.print_trainable_parameters()
    print("å½“å‰è®­ç»ƒçŠ¶æ€: ä»ç¬¬ 4 è½®å¼€å§‹è¿è¡Œ...")

# ================= 4. æ•°æ®æ ¼å¼åŒ– (ä¿æŒä¸å˜) =================
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are an expert in gender bias mitigation. Please analyze the following text for gender bias, provide a step-by-step chain-of-thought analysis, classify the bias type, and provide a rewritten version if bias exists.

### Input:
{}

### Response:
{}"""

def preprocess_function(examples):
    inputs = examples["original_text"]
    targets = examples["Bias_Analysis_CoT"]
    model_inputs = []
    for i in range(len(inputs)):
        prompt = alpaca_prompt.format(inputs[i], "")
        full_text = prompt + str(targets[i]) + tokenizer.eos_token
        tokenized = tokenizer(full_text, truncation=True, max_length=2048, padding="max_length")
        tokenized["labels"] = tokenized["input_ids"].copy()
        model_inputs.append(tokenized)
    return {k: [d[k] for d in model_inputs] for k in model_inputs[0].keys()}

tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)

# ================= 5. å¼€å§‹ç»­è®­ (æœ€ç»ˆä¿®å¤) =================
if local_rank == 0:
    print("\n=== å¯åŠ¨ç»­è®­ (ç›®æ ‡ï¼šå†è·‘ 7 è½®) ===")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2, 
    gradient_accumulation_steps=8, 
    learning_rate=2e-4,
    num_train_epochs=7, # è·‘å‰©ä¸‹çš„7è½®
    logging_steps=10,
    
    # âŒ æ ¸å¿ƒä¿®æ”¹ï¼šå…³é—­ fp16ï¼Œæ”¹ç”¨ fp32
    fp16=False, 
    
    save_strategy="epoch",
    save_total_limit=2,
    optim="adamw_torch",
    report_to="none",
    ddp_find_unused_parameters=False, 
    gradient_checkpointing=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True),
)

# ğŸ”¥ æ ¸å¿ƒå¯åŠ¨ï¼šä¸ä½¿ç”¨ resume_from_checkpoint=True
# å› ä¸ºæ¨¡å‹å·²ç»æ‰‹åŠ¨åŠ è½½äº†æœ€æ–°çš„æƒé‡ï¼Œç›´æ¥å¼€å§‹æ–°çš„ Trainer å³å¯
trainer.train()

# ================= 6. ä¿å­˜æœ€ç»ˆç»“æœ =================
if local_rank == 0:
    print(f"\n=== 10è½®è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {OUTPUT_DIR} ===")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("ğŸ‰ ç»­è®­ä»»åŠ¡åœ†æ»¡ç»“æŸï¼")