import torch
import json
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen2Config
from peft import PeftModel

# ================= é…ç½®åŒºåŸŸ =================
BASE_MODEL_PATH = "/public/home/lilingzhi/Qwen2.5-7B-Instruct"  # ä½ çš„åº•åº§æ¨¡å‹è·¯å¾„
ADAPTER_PATH = "qwen_lora_outputs_ddp"  # åˆšæ‰è®­ç»ƒä¿å­˜çš„LoRAè·¯å¾„
TEST_DATA_FILE = "pianjian_cot_backup.json"  # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç”¨è®­ç»ƒæ•°æ®æµ‹ï¼Œå®é™…åº”æ¢æˆéªŒè¯é›†
OUTPUT_FILE = "inference_results.json"

# ================= 1. åŠ è½½æ¨¡å‹ =================
print("=== æ­£åœ¨åŠ è½½æ¨¡å‹ä¸LoRAé€‚é…å™¨ ===")

# åŠ è½½ Config
config = Qwen2Config.from_pretrained(BASE_MODEL_PATH, local_files_only=True)

# åŠ è½½åº•åº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    config=config,
    torch_dtype=torch.float16,
    device_map="auto",
    local_files_only=True
)

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

# åŠ è½½ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, local_files_only=True)

# ================= 2. å‡†å¤‡æµ‹è¯•æ•°æ® =================
print(f"\n=== æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {TEST_DATA_FILE} ===")
# è¯»å–å‰ 5 æ¡åšæ¼”ç¤º
try:
    df = pd.read_json(TEST_DATA_FILE, lines=True)
    test_samples = df.head(5).to_dict(orient="records")
except ValueError:
    df = pd.read_json(TEST_DATA_FILE)
    test_samples = df.head(5).to_dict(orient="records")

# ================= 3. å¼€å§‹æ¨ç† =================
print("\n=== å¼€å§‹æ¨ç† (æµ‹è¯•5æ¡) ===")

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are an expert in gender bias mitigation. Please analyze the following text for gender bias, provide a step-by-step chain-of-thought analysis, classify the bias type, and provide a rewritten version if bias exists.

### Input:
{}

### Response:
"""

results = []

with torch.no_grad():
    for item in tqdm(test_samples):
        input_text = item["original_text"]
        
        # æ„é€  Prompt
        prompt = alpaca_prompt.format(input_text)
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        
        # ç”Ÿæˆå›ç­”
        outputs = model.generate(
            **inputs, 
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9
        )
        
        # è§£ç å¹¶æå–å›å¤éƒ¨åˆ†
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ç®€å•æå– Response: ä¹‹åçš„å†…å®¹
        if "### Response:" in response:
            generated_text = response.split("### Response:")[-1].strip()
        else:
            generated_text = response

        print(f"\n[åŸæ–‡]: {input_text}")
        print(f"[æ¨¡å‹ç”Ÿæˆ]: {generated_text[:100]}...") # åªæ‰“å°å‰100å­—é¢„è§ˆ
        
        results.append({
            "original_text": input_text,
            "generated_analysis": generated_text,
            "ground_truth": item.get("Bias_Analysis_CoT", "")
        })

# ================= 4. ä¿å­˜ç»“æœ =================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"\nğŸ‰ æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_FILE}")