import pandas as pd
import json
import torch
import dashscope
from transformers import BertTokenizer, BertForTokenClassification
from py2neo import Graph
from http import HTTPStatus

# ==================== é…ç½®åŒºåŸŸ (è¯·ä¿®æ”¹è¿™é‡Œ) ====================
# 1. å¡«å…¥ä½ çš„é˜¿é‡Œ DashScope API Key
dashscope.api_key = "sk-1a5fe8ff79f24ba88eef4ef7c52f60c1"  # <--- æŠŠä½ çš„Keyç²˜è´´åœ¨è¿™é‡Œ

# 2. ä½ çš„ Neo4j å¯†ç 
NEO4J_PASSWORD = "zjt20050213"  # <--- å¡«å…¥ä½ çš„Neo4jå¯†ç 

# 3. Excel æ–‡ä»¶é…ç½®
EXCEL_FILE = 'D:\\è¯¾ç¨‹èµ„æ–™\\æœºå™¨å­¦ä¹ å®è·µ\\æœŸæœ«å¤§ä½œä¸š\\æœŸæœ«å¤§ä½œä¸š\\åŒ»ç–—é—®ç­”\\questions.csv'  # å‡è®¾ä½ çš„æ–‡ä»¶åæ˜¯è¿™ä¸ªï¼Œå¦‚æœæ˜¯csvè¯·æ”¹åç¼€
COLUMN_NAME = 'content'       # <--- è¯·ç¡®è®¤ä½ Excelé‡Œå­˜æ”¾é—®é¢˜çš„åˆ—å(è¡¨å¤´)å«ä»€ä¹ˆï¼Ÿ

# 4. æ¨¡å‹é€‰æ‹© (è¿™é‡ŒæŒ‡å®šäº†ä½¿ç”¨ qwen-plus)
LLM_MODEL = "qwen-plus"
# ============================================================

# è¿æ¥ Neo4j
try:
    graph = Graph("bolt://localhost:7687", auth=("neo4j", NEO4J_PASSWORD))
    print("âœ… Neo4j è¿æ¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ Neo4j è¿æ¥å¤±è´¥: {e}")
    exit()

# åŠ è½½ NER æ¨¡å‹
print("æ­£åœ¨åŠ è½½ NER æ¨¡å‹...")
MODEL_PATH = './saved_model'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

try:
    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
    model = BertForTokenClassification.from_pretrained(MODEL_PATH)
    model.to(device)
    model.eval()
    
    # åŠ è½½æ ‡ç­¾æ˜ å°„ (éœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´)
    # è¿™é‡Œæˆ‘ä»¬ç¡¬ç¼–ç è®­ç»ƒæ—¶çš„æ ‡ç­¾åˆ—è¡¨ï¼Œç¡®ä¿å¯¹åº”æ­£ç¡®
    # æ³¨æ„ï¼šå¦‚æœä½ çš„æ ‡ç­¾åˆ—è¡¨é¡ºåºå˜äº†ï¼Œè¿™é‡Œéœ€è¦è°ƒæ•´ã€‚æœ€ç¨³å¦¥çš„æ˜¯ä¿å­˜è®­ç»ƒæ—¶çš„tag2idã€‚
    # æ ¹æ®ä½ ä¹‹å‰çš„æˆªå›¾ï¼Œè¿™æ˜¯ä½ è®­ç»ƒæ—¶çš„æ ‡ç­¾é¡ºåºï¼š
    labels_list = ['B-æ£€æŸ¥é¡¹ç›®', 'B-æ²»ç–—æ–¹æ³•', 'B-ç–¾ç—…', 'B-ç–¾ç—…ç—‡çŠ¶', 'B-ç§‘ç›®', 'B-è¯å“', 'B-è¯å“å•†', 'B-é£Ÿç‰©', 
                   'I-æ£€æŸ¥é¡¹ç›®', 'I-æ²»ç–—æ–¹æ³•', 'I-ç–¾ç—…', 'I-ç–¾ç—…ç—‡çŠ¶', 'I-ç§‘ç›®', 'I-è¯å“', 'I-è¯å“å•†', 'I-é£Ÿç‰©', 'O']
    id2tag = {i: tag for i, tag in enumerate(labels_list)}
    print("âœ… NER æ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit()

# NER æ¨ç†å‡½æ•°
def extract_entities(text):
    tokens = [tokenizer.cls_token_id]
    token_list = []
    for char in text:
        token_list.append(char)
        tokens.extend(tokenizer.encode(char, add_special_tokens=False))
    tokens.append(tokenizer.sep_token_id)
    
    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)
    with torch.no_grad():
        outputs = model(input_ids)
    
    logits = outputs.logits
    preds = torch.argmax(logits, dim=2).cpu().numpy()[0]
    
    # è§£æ BIO æ ‡ç­¾
    entities = {}
    curr_entity = ""
    curr_type = ""
    
    # ä¸ºäº†å¯¹é½ï¼Œå»æ‰å¤´å°¾çš„ [CLS] [SEP]
    preds = preds[1:-1]
    
    # ç®€å•çš„å®ä½“æå–é€»è¾‘
    for char, tag_id in zip(token_list, preds):
        if tag_id >= len(labels_list): continue
        tag = id2tag[tag_id]
        
        if tag.startswith("B-"):
            if curr_entity: # ä¿å­˜ä¸Šä¸€ä¸ª
                if curr_type not in entities: entities[curr_type] = []
                entities[curr_type].append(curr_entity)
            curr_entity = char
            curr_type = tag.split("-")[1]
        elif tag.startswith("I-") and curr_type == tag.split("-")[1]:
            curr_entity += char
        else:
            if curr_entity:
                if curr_type not in entities: entities[curr_type] = []
                entities[curr_type].append(curr_entity)
            curr_entity = ""
            curr_type = ""
            
    if curr_entity:
        if curr_type not in entities: entities[curr_type] = []
        entities[curr_type].append(curr_entity)
        
    return entities

# å¤§æ¨¡å‹è°ƒç”¨å‡½æ•°
def call_llm(prompt):
    messages = [{'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†å›¾è°±é—®ç­”åŠ©æ‰‹ã€‚'},
                {'role': 'user', 'content': prompt}]
    
    try:
        response = dashscope.Generation.call(
            model=LLM_MODEL,  # <--- è¿™é‡ŒæŒ‡å®šäº†ä½¿ç”¨ qwen-plus
            messages=messages,
            result_format='message',
        )
        if response.status_code == HTTPStatus.OK:
            return response.output.choices[0]['message']['content']
        else:
            return f"Error: {response.code} - {response.message}"
    except Exception as e:
        return f"Exception: {e}"

# è§£ææ„å›¾å¹¶æŸ¥è¯¢ Neo4j
def execute_query(intent_str):
    # PPTè¦æ±‚çš„æ ¼å¼æ˜¯: "1 ç–¾ç—…åç§° å±æ€§" æˆ– "2 ç–¾ç—…åç§° å…³ç³» å®ä½“ç±»åˆ«"
    # æˆ‘ä»¬éœ€è¦è§£æè¿™ä¸ªå­—ç¬¦ä¸²å¹¶ç”Ÿæˆ Cypher
    results = []
    queries = intent_str.split(',') # å¤šä¸ªæŸ¥è¯¢ç”¨é€—å·éš”å¼€
    
    for q in queries:
        q = q.strip()
        parts = q.split()
        if len(parts) < 3: continue
        
        q_type = parts[0]
        name = parts[1]
        
        cypher = ""
        try:
            if q_type == '1': # æŸ¥è¯¢å±æ€§
                attr = parts[2]
                # å±æ€§åæ˜ å°„ (PPT 32é¡µå¯¹åº”)
                attr_map = {
                    "ç–¾ç—…ç®€ä»‹": "desc", "ç–¾ç—…ç—…å› ": "cause", "é¢„é˜²æªæ–½": "prevent",
                    "æ²»ç–—å‘¨æœŸ": "cure_lasttime", "æ²»æ„ˆæ¦‚ç‡": "cured_prob", "ç–¾ç—…æ˜“æ„Ÿäººç¾¤": "easy_get"
                }
                db_attr = attr_map.get(attr, attr) # æ‰¾ä¸åˆ°å°±ç”¨åŸå
                cypher = f"MATCH (n:Disease {{name: '{name}'}}) RETURN n.{db_attr} as result"
                
            elif q_type == '2': # æŸ¥è¯¢å…³ç³»
                # æ ¼å¼: 2 ç–¾ç—…åç§° å…³ç³»åç§° å®ä½“ç±»åˆ«
                if len(parts) >= 4:
                    rel_name = parts[2]
                    target_label = parts[3]
                    # å®ä½“ç±»åˆ«æ˜ å°„ (è‹±æ–‡)
                    label_map = {
                        "è¯å“": "Drug", "é£Ÿç‰©": "Food", "æ£€æŸ¥é¡¹ç›®": "Check", 
                        "ç§‘ç›®": "Department", "ç–¾ç—…ç—‡çŠ¶": "Symptom", "æ²»ç–—æ–¹æ³•": "CureWay", "ç–¾ç—…": "Disease"
                    }
                    target_label_en = label_map.get(target_label, "Node")
                    cypher = f"MATCH (n:Disease {{name: '{name}'}})-[:{rel_name}]->(m:{target_label_en}) RETURN m.name as result"
            
            if cypher:
                print(f"  [Cypher]: {cypher}")
                data = graph.run(cypher).data()
                results.append(data)
                
        except Exception as e:
            print(f"  [Query Error]: {e}")
            
    return results

# ==================== ä¸»æµç¨‹ ====================
def main():
    # 1. è¯»å– Excel
    print(f"æ­£åœ¨è¯»å– {EXCEL_FILE}...")
    try:
        df = pd.read_csv(EXCEL_FILE)
        # å¦‚æœæ‰¾ä¸åˆ°æŒ‡å®šåˆ—ï¼Œå°è¯•ç”¨ç¬¬ä¸€åˆ—
        if COLUMN_NAME not in df.columns:
            print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°åˆ—å '{COLUMN_NAME}'ï¼Œå°†é»˜è®¤ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºé—®é¢˜åˆ—ã€‚")
            questions = df.iloc[:, 0].astype(str).tolist()
        else:
            questions = df[COLUMN_NAME].astype(str).tolist()
    except Exception as e:
        print(f"âŒ è¯»å– Excel å¤±è´¥: {e}")
        return

    # åªå–å‰ 100 æ¡ (PPTè¦æ±‚)
    questions = questions[:100]
    final_output = []

    # 2. å¾ªç¯å¤„ç†
    print(f"å¼€å§‹å¤„ç† {len(questions)} ä¸ªé—®é¢˜...")
    
    # PPT Source 49 æä¾›çš„ Prompt æ¨¡ç‰ˆ
    PROMPT_TEMPLATE = """
    ç°åœ¨ï¼Œä½ æ˜¯ä¸€ä¸ªæœºå™¨äººåŒ»ç”Ÿï¼Œç”¨æˆ·å¯¹ä½ è¾“å…¥é—®é¢˜ï¼Œä½ éœ€è¦ç²¾å‡†çš„ç†è§£é—®é¢˜çš„å†…å®¹ï¼Œæ ¹æ®å…¶å«ä¹‰æ„å»ºNeo4jæ•°æ®åº“çš„æŸ¥è¯¢è¯­å¥...
    (æ­¤å¤„çœç•¥é•¿ Promptï¼Œä¸ºäº†ä»£ç æ•´æ´ï¼Œæˆ‘ä»¬ç”¨ç®€åŒ–çš„æ ¸å¿ƒé€»è¾‘ï¼Œå®é™…è¿è¡Œæ—¶è¯·æŠŠ PPT 49 é¡µå®Œæ•´çš„ Prompt æ–‡å­—è´´åœ¨è¿™é‡Œï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ä¸‹é¢çš„ç²¾ç®€ç‰ˆ)
    
    æç¤º:ç›®å‰æˆ‘çš„å›¾æ•°æ®åº“ä¸­æœ‰8ç±»å®ä½“: ç–¾ç—…ã€è¯å“ã€è¯å“å•†ã€ç–¾ç—…ç—‡çŠ¶ã€é£Ÿç‰©ã€æ£€æŸ¥é¡¹ç›®ã€æ²»ç–—æ–¹æ³•ã€ç§‘ç›®ã€‚
    æŸ¥è¯¢è¯­å¥æ ¼å¼åº”ä¸º: 
    ç±»å‹1(æŸ¥è¯¢å±æ€§): "1 ç–¾ç—…åç§° å±æ€§å" (å±æ€§åŒ…æ‹¬: ç–¾ç—…ç®€ä»‹, ç–¾ç—…ç—…å› , é¢„é˜²æªæ–½, æ²»ç–—å‘¨æœŸ, æ²»æ„ˆæ¦‚ç‡, ç–¾ç—…æ˜“æ„Ÿäººç¾¤)
    ç±»å‹2(æŸ¥è¯¢å…³ç³»): "2 ç–¾ç—…åç§° å…³ç³»åç§° å®ä½“ç±»åˆ«" (å…³ç³»åŒ…æ‹¬: ç–¾ç—…ä½¿ç”¨è¯å“, ç–¾ç—…å®œåƒé£Ÿç‰©, ç–¾ç—…å¿Œåƒé£Ÿç‰©, ç–¾ç—…æ‰€éœ€æ£€æŸ¥, ç–¾ç—…æ‰€å±ç§‘ç›®, ç–¾ç—…çš„ç—‡çŠ¶, æ²»ç–—çš„æ–¹æ³•, ç–¾ç—…å¹¶å‘ç–¾ç—…)
    
    ç”¨æˆ·é—®é¢˜: {question}
    
    è¯·ç›´æ¥è¾“å‡ºæŸ¥è¯¢è¯­å¥ï¼Œä¸è¦è¾“å‡ºå…¶ä»–åºŸè¯ã€‚å¦‚æœæœ‰å¤šä¸ªæŸ¥è¯¢ç”¨é€—å·éš”å¼€ã€‚
    ä¾‹å¦‚: 1 å£è‡­ ç–¾ç—…ç®€ä»‹, 2 å£è‡­ æ²»ç–—çš„æ–¹æ³• æ²»ç–—æ–¹æ³•
    """

    for i, q in enumerate(questions):
        print(f"\n--- å¤„ç†ç¬¬ {i+1} æ¡: {q} ---")
        
        # æ­¥éª¤ A: NER è¯†åˆ«
        entities = extract_entities(q)
        print(f"  [NER]: {entities}")
        
        # æ­¥éª¤ B: æ„é€  Prompt å¹¶è°ƒç”¨ LLM è·å–æ„å›¾
        # å°† NER ç»“æœä¹Ÿæ”¾å…¥ Prompt è¾…åŠ©æ¨¡å‹ (å¯é€‰)
        prompt = PROMPT_TEMPLATE.format(question=q)
        llm_res = call_llm(prompt)
        print(f"  [LLM Intent]: {llm_res}")
        
        # æ­¥éª¤ C: è§£ææ„å›¾å¹¶æŸ¥è¯¢ Graph
        db_results = execute_query(llm_res)
        print(f"  [DB Result]: {db_results}")
        
        # æ­¥éª¤ D: ä¿å­˜ç»“æœ
        item = {
            "id": i,
            "question": q,
            "ner_results": entities,
            "intent_raw": llm_res,
            "query_results": db_results
        }
        final_output.append(item)

    # 3. å†™å…¥æ–‡ä»¶
    with open('final_result.json', 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False, indent=4)
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç»“æœå·²ä¿å­˜ä¸º final_result.json")

if __name__ == "__main__":
    main()