import streamlit as st
import torch
import dashscope
from transformers import AutoTokenizer, AutoModelForTokenClassification
from py2neo import Graph
from http import HTTPStatus

# ================= é…ç½®åŒºåŸŸ =================
dashscope.api_key = "sk-1a5fe8ff79f24ba88eef4ef7c52f60c1" # <--- å¡«ä½ çš„ Key
NEO4J_PASSWORD = "zjt20050213"                       # <--- å¡«ä½ çš„å¯†ç 
MODEL_PATH = './saved_model'
# ============================================

# é¡µé¢æ ‡é¢˜
st.set_page_config(page_title="åŒ»ç–—çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ", page_icon="ğŸ¥")
st.title("ğŸ¥ æ™ºèƒ½åŒ»ç–—é—®ç­”åŠ©æ‰‹")
st.markdown("åŸºäº **çŸ¥è¯†å›¾è°± (Neo4j)** + **å¤§æ¨¡å‹ (Qwen-Plus)** + **NER (BERT)**")

# 1. åˆå§‹åŒ–è¿æ¥ (åŠ ç¼“å­˜ï¼ŒåªåŠ è½½ä¸€æ¬¡)
@st.cache_resource
def init_resources():
    # è¿æ¥ Neo4j
    try:
        graph = Graph("bolt://localhost:7687", auth=("neo4j", NEO4J_PASSWORD))
    except:
        return None, None, None, None
    
    # åŠ è½½æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)
    model.to(device)
    model.eval()
    
    # æ ‡ç­¾æ˜ å°„
    labels_list = ['B-æ£€æŸ¥é¡¹ç›®', 'B-æ²»ç–—æ–¹æ³•', 'B-ç–¾ç—…', 'B-ç–¾ç—…ç—‡çŠ¶', 'B-ç§‘ç›®', 'B-è¯å“', 'B-è¯å“å•†', 'B-é£Ÿç‰©', 
                   'I-æ£€æŸ¥é¡¹ç›®', 'I-æ²»ç–—æ–¹æ³•', 'I-ç–¾ç—…', 'I-ç–¾ç—…ç—‡çŠ¶', 'I-ç§‘ç›®', 'I-è¯å“', 'I-è¯å“å•†', 'I-é£Ÿç‰©', 'O']
    id2tag = {i: tag for i, tag in enumerate(labels_list)}
    
    return graph, tokenizer, model, id2tag, device

graph, tokenizer, model, id2tag, device = init_resources()

if graph is None:
    st.error("âŒ æ•°æ®åº“æˆ–æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")
    st.stop()

# ä¾§è¾¹æ 
with st.sidebar:
    st.success("âœ… ç³»ç»ŸçŠ¶æ€ï¼šåœ¨çº¿")
    st.info("ğŸ’¡ æç¤ºï¼šè¯•ç€é—®é—® 'æ„Ÿå†’äº†åƒä»€ä¹ˆè¯ï¼Ÿ' æˆ– 'é«˜è¡€å‹æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ'")

# 2. æ ¸å¿ƒå‡½æ•° (ç›´æ¥å¤ç”¨ä½ ä¹‹å‰çš„é€»è¾‘)
def extract_entities(text):
    tokens = [tokenizer.cls_token_id]
    token_list = []
    for char in text:
        token_list.append(char)
        tokens.extend(tokenizer.encode(char, add_special_tokens=False))
    tokens.append(tokenizer.sep_token_id)
    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)
    
    with torch.no_grad():
        outputs = model(input_ids)
    preds = torch.argmax(outputs.logits, dim=2).cpu().numpy()[0][1:-1]
    
    entities = {}
    curr_entity = ""
    curr_type = ""
    for char, tag_id in zip(token_list, preds):
        if tag_id >= len(id2tag): continue
        tag = id2tag[tag_id]
        if tag.startswith("B-"):
            if curr_entity:
                if curr_type not in entities: entities[curr_type] = []
                entities[curr_type].append(curr_entity)
            curr_entity = char
            curr_type = tag.split("-")[1]
        elif tag.startswith("I-") and curr_type == tag.split("-")[1]:
            curr_entity += char
        else:
            if curr_entity:
                if curr_type not in entities: entities[curr_type] = []
                entities[curr_type].append(curr_entity)
            curr_entity = ""; curr_type = ""
    if curr_entity:
        if curr_type not in entities: entities[curr_type] = []
        entities[curr_type].append(curr_entity)
    return entities

def get_answer(question):
    # NER
    entities = extract_entities(question)
    
    # LLM Intent
    prompt = f"""
    ç°åœ¨ï¼Œä½ æ˜¯ä¸€ä¸ªæœºå™¨äººåŒ»ç”Ÿã€‚ç›®å‰æˆ‘çš„å›¾æ•°æ®åº“ä¸­æœ‰8ç±»å®ä½“: ç–¾ç—…ã€è¯å“ã€è¯å“å•†ã€ç–¾ç—…ç—‡çŠ¶ã€é£Ÿç‰©ã€æ£€æŸ¥é¡¹ç›®ã€æ²»ç–—æ–¹æ³•ã€ç§‘ç›®ã€‚
    æŸ¥è¯¢è¯­å¥æ ¼å¼åº”ä¸º: 
    ç±»å‹1(æŸ¥è¯¢å±æ€§): "1 ç–¾ç—…åç§° å±æ€§å" (å±æ€§åŒ…æ‹¬: ç–¾ç—…ç®€ä»‹, ç–¾ç—…ç—…å› , é¢„é˜²æªæ–½, æ²»ç–—å‘¨æœŸ, æ²»æ„ˆæ¦‚ç‡, ç–¾ç—…æ˜“æ„Ÿäººç¾¤)
    ç±»å‹2(æŸ¥è¯¢å…³ç³»): "2 ç–¾ç—…åç§° å…³ç³»åç§° å®ä½“ç±»åˆ«" (å…³ç³»åŒ…æ‹¬: ç–¾ç—…ä½¿ç”¨è¯å“, ç–¾ç—…å®œåƒé£Ÿç‰©, ç–¾ç—…å¿Œåƒé£Ÿç‰©, ç–¾ç—…æ‰€éœ€æ£€æŸ¥, ç–¾ç—…æ‰€å±ç§‘ç›®, ç–¾ç—…çš„ç—‡çŠ¶, æ²»ç–—çš„æ–¹æ³•, ç–¾ç—…å¹¶å‘ç–¾ç—…)
    ç”¨æˆ·é—®é¢˜: {question}
    è¯·ç›´æ¥è¾“å‡ºæŸ¥è¯¢è¯­å¥ï¼Œå¤šä¸ªç”¨é€—å·éš”å¼€ã€‚
    """
    messages = [{'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†å›¾è°±é—®ç­”åŠ©æ‰‹ã€‚'}, {'role': 'user', 'content': prompt}]
    resp = dashscope.Generation.call(model="qwen-plus", messages=messages, result_format='message')
    intent_str = resp.output.choices[0]['message']['content']
    
    # Graph Query
    results = []
    queries = intent_str.split(',')
    for q in queries:
        parts = q.strip().split()
        if len(parts) < 3: continue
        cypher = ""
        try:
            name = parts[1]
            if parts[0] == '1':
                attr_map = {"ç–¾ç—…ç®€ä»‹": "desc", "ç–¾ç—…ç—…å› ": "cause", "é¢„é˜²æªæ–½": "prevent", "æ²»ç–—å‘¨æœŸ": "cure_lasttime", "æ²»æ„ˆæ¦‚ç‡": "cured_prob"}
                attr = attr_map.get(parts[2], parts[2])
                cypher = f"MATCH (n:Disease {{name: '{name}'}}) RETURN n.{attr} as result"
            elif parts[0] == '2' and len(parts) >= 4:
                rel = parts[2]
                target_map = {"è¯å“": "Drug", "é£Ÿç‰©": "Food", "æ£€æŸ¥é¡¹ç›®": "Check", "ç§‘ç›®": "Department", "ç–¾ç—…ç—‡çŠ¶": "Symptom", "æ²»ç–—æ–¹æ³•": "CureWay", "ç–¾ç—…": "Disease"}
                target = target_map.get(parts[3], "Node")
                cypher = f"MATCH (n:Disease {{name: '{name}'}})-[:{rel}]->(m:{target}) RETURN m.name as result"
            
            if cypher:
                data = graph.run(cypher).data()
                if data: results.append(f"ã€{name}ã€‘: {str([d['result'] for d in data])}")
        except: continue
        
    return entities, intent_str, results

# 3. èŠå¤©ç•Œé¢é€»è¾‘
# åˆå§‹åŒ–èŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# æ¥æ”¶ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # å¤„ç†å¹¶å›å¤
    with st.chat_message("assistant"):
        with st.status("æ­£åœ¨æ€è€ƒä¸­...", expanded=True):
            st.write("ğŸ” æ­£åœ¨è¿›è¡Œå‘½åå®ä½“è¯†åˆ«...")
            entities, intent, answers = get_answer(prompt)
            st.write(f"ğŸ·ï¸ è¯†åˆ«å®ä½“: {entities}")
            
            st.write("ğŸ§  æ­£åœ¨åˆ†ææ„å›¾ (LLM)...")
            st.write(f"ğŸ¯ æŸ¥è¯¢æŒ‡ä»¤: {intent}")
            
            st.write("ğŸ•¸ï¸ æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†å›¾è°±...")
            
        if answers:
            response = "æ ¹æ®çŸ¥è¯†åº“æŸ¥è¯¢ï¼Œç»“æœå¦‚ä¸‹ï¼š\n\n" + "\n".join(answers)
        else:
            response = "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æš‚æ—¶æ²¡æœ‰æŸ¥åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæˆ–è€…è¯¥é—®é¢˜ä¸å±äºåŒ»ç–—çŸ¥è¯†å›¾è°±èŒƒå›´ã€‚"
            
        st.markdown(response)
    
    st.session_state.messages.append({"role": "assistant", "content": response})